{"paragraphs":[{"text":"%spark.pyspark\nimport os\nimport urllib.request\nimport zipfile\nimport json\nimport logging\njob_name = \"acs_data\"\n\nformatter = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - ln%(lineno)d - %(message)s'\nlogging.basicConfig(format=formatter)\nlogger = logging.getLogger(job_name)\nlogger.setLevel(logging.DEBUG)\n","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1597655514393_-458975244","id":"20200603-130026_1672067123","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:10:56+0000","dateFinished":"2020-08-19T10:10:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:492"},{"text":"%spark.pyspark\r\n\r\nfrom pyspark.sql import SparkSession\r\nspark = SparkSession.builder.appName('acsdata').getOrCreate()\r\nes_hadoop = urllib.request.URLopener()\r\nes_hadoop.retrieve(\"http://download.elastic.co/hadoop/elasticsearch-hadoop-6.1.1.zip\", \"es-hadoop.zip\")\r\nwith zipfile.ZipFile(\"es-hadoop.zip\",\"r\") as zip_ref:\r\n    zip_ref.extractall()\r\n    \r\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-6.1.1/dist/elasticsearch-spark-20_2.11-6.1.1.jar pyspark-shell'","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1597655514398_-1326501058","id":"20200603-140303_2021249394","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:09:10+0000","dateFinished":"2020-08-19T10:09:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:493"},{"text":"%spark.pyspark\r\n\r\nfrom pyspark.sql import SparkSession\r\nspark = SparkSession.builder.appName('acsdata').getOrCreate()","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1597655514398_-25955647","id":"20200605-065932_541237539","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:09:11+0000","dateFinished":"2020-08-19T10:09:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:494"},{"text":"%spark.pyspark\r\n\r\nes_write_conf_acs = {\r\n    \"es.nodes\" : \"https://vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\r\n    \"es.port\" : \"443\",\r\n    \"es.net.ssl\": \"true\",\r\n    \"es.nodes.wan.only\": \"true\",\r\n    \"es.resource\" : \"acsdata/data\",\r\n    \"es.input.json\" : \"yes\",\r\n\t\"es.mapping.id\": \"id\"\r\n}","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1597655514398_-2085004607","id":"20200605-074251_246399432","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:09:11+0000","dateFinished":"2020-08-19T10:09:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:495"},{"text":"%spark.pyspark\r\ndf = spark.read.json('s3://271849239056-dev-datagov-stage-s3/datasource=acs/acs-pyxis/B25064')\r\ndf.printSchema()","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597655514399_-1857995570","id":"20200605-075313_120424338","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:09:11+0000","dateFinished":"2020-08-19T10:09:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:496"},{"text":"%spark.pyspark\ndef generate_Additional_Fields(geoId, vintage, variableCode):\n    zipCode = geoId.split('US')[1]\n    code = variableCode.split('_')\n    id = '{}-{}-{}'.format(zipCode, vintage, variableCode)\n    return {'id': id, 'zipCode': zipCode, 'variableCode': code[0], 'variableCodeType': code[1]}","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1597655514399_-662420365","id":"20200605-080634_1833238006","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:09:20+0000","dateFinished":"2020-08-19T10:09:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:497"},{"text":"%spark.pyspark\ndef acs_formatter(x, sc):\n    additional_Fields = generate_Additional_Fields(x['geoId'],\n                                                   x['vintage'],\n                                                   x['variableCode'])\n    if x['vintage'] == 2018:\n        calculated_value = create_calculated_value(additional_Fields['variableCode'],\n                                                   additional_Fields['variableCodeType'],\n                                                   additional_Fields['zipCode'],\n                                                   x['vintage'],\n                                                   x['estimateValue'],\n                                                   sc)\n    else:\n        calculated_value = x['estimateValue']\n\n    id = additional_Fields['id']\n    return (id, json.dumps({\n        'id': id,\n        'geoId': additional_Fields['zipCode'],\n        'variableCodeType': additional_Fields['variableCodeType'],\n        'variableCode': additional_Fields['variableCode'],\n        'estimateValue':  x['estimateValue'],\n        'calculatedValue': calculated_value,\n        'vintage':  x['vintage']\n    })\n    )","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1597655514399_-1183165715","id":"20200605-080702_2043374992","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:09:20+0000","dateFinished":"2020-08-19T10:09:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:498"},{"text":"%spark.pyspark\ndef create_calculated_value(variableCode, variableCodeType, zipCode, vintage, estimatedValue, sc):\n    if(variableCode == \"B25119\"):\n        return estimatedValue\n    if(variableCode == \"B25064\"):\n        query = {\n            \"query\": {\n                \"bool\": {\n                    \"filter\": [\n                     {\n                         \"bool\": {\n                             \"must\": [\n                                 {\n                                     \"match\": {\n                                         \"variableCode\": \"B25119\"\n                                     }\n                                 },\n                                 {\n                                     \"match\": {\n                                         \"variableCodeType\": \"003\"\n                                     }\n                                 },\n                                 {\n                                     \"match\": {\n                                         \"geoId\": zipCode\n                                     }\n                                 },\n                                 {\n                                     \"match\": {\n                                         \"vintage\": vintage\n                                     }\n                                 }\n                             ]\n                         }\n                     }\n                    ]\n                }\n            }\n        }\n        es_read_conf = {\n            \"es.nodes\": \"https://vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\n            \"es.port\": \"443\",\n            \"es.resource\": \"pyxis-demographics/demographics\",\n            \"es.nodes.wan.only\": \"true\",\n            \"es.query\": json.dumps(query)\n        }\n        es_rdd = sc.newAPIHadoopRDD(\n            inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n            keyClass=\"org.apache.hadoop.io.NullWritable\",\n            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n            conf=es_read_conf)\n        try:\n            calculated_value = (12*estimatedValue)/es_rdd.collect()[0][1]['estimateValue']\n            print('calculated value-->',calculated_value)\n            return calculated_value\n        except Exception as e:\n            logger.error(\"Error in calculation{}\".format(e))","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1597655514399_-518795617","id":"20200605-080715_1536189257","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:22:07+0000","dateFinished":"2020-08-19T10:22:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:499"},{"text":"%spark.pyspark\n es_write_conf_acs = {\n             \"es.nodes\" : \"https://vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\n            \"es.port\": \"443\",\n            \"es.net.ssl\": \"true\",\n            \"es.nodes.wan.only\": \"true\",\n            \"es.resource\": \"pyxis-demographics/demographics\",\n            \"es.input.json\": \"true\",\n            \"es.mapping.id\": \"id\"\n        }","user":"anonymous","dateUpdated":"2020-08-19T11:08:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1597655514399_1274621706","id":"20200605-080727_328263989","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:09:20+0000","dateFinished":"2020-08-19T10:09:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:500"},{"text":"%spark.pyspark\n#  df = sh.ssc.read.json(\"{}/{}\".format(mh.bucket_key, \"B25064\"))\ndf = spark.read.json('s3://271849239056-dev-datagov-stage-s3/datasource=acs/acs-pyxis/B25064')\ndf.printSchema()\nformattedata = []\nformatedRdd=[]\nfor row in df.rdd.toLocalIterator():\n    if len(formattedata) == 50:\n        formatedRdd = sc.parallelize(formattedata)\n        print('formatedRdd-->',formatedRdd.take(1))\n        formatedRdd.saveAsNewAPIHadoopFile(\n            path='-',\n            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n            keyClass=\"org.apache.hadoop.io.NullWritable\",\n            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n            conf=es_write_conf_acs)\n        formattedata = []\n        formatedRdd = []\n        print(\" Job {} inserted 5000 records\".format(job_name))\n    else:\n        obj = acs_formatter(row, sc)\n        formattedata.append(obj)\n\nformatedRdd = sh.sc.parallelize(formattedata)\nformatedRdd.saveAsNewAPIHadoopFile(\n    path='-',\n    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n    keyClass=\"org.apache.hadoop.io.NullWritable\",\n    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n    conf=es_write_conf_acs)\nformattedata = []\nformatedRdd = []\nlogger.info(\n    \"Job {} inserted all records shutting down\".format(job_name))","user":"anonymous","dateUpdated":"2020-08-19T11:13:59+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597655514400_835984618","id":"20200605-080735_187928323","dateCreated":"2020-08-17T09:11:54+0000","dateStarted":"2020-08-19T10:24:53+0000","dateFinished":"2020-08-19T10:33:28+0000","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:501"},{"text":"%spark.pyspark\nimport logging\nimport argparse\nimport json\nfrom pyspark.sql.functions import col\nfrom helper_files.aws_helper import AWSHelper\nfrom helper_files.spark_helper import SparkHelper\nimport os\n\njob_name = \"acs_data\"\n\nformatter = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - ln%(lineno)d - %(message)s'\nlogging.basicConfig(format=formatter)\nlogger = logging.getLogger(job_name)\nlogger.setLevel(logging.DEBUG)\n\n\nclass MapHelper:\n    def __init__(self, job_execution_parameters):\n        self.job_execution_parameters = job_execution_parameters\n        self.datasource_name = self.job_execution_parameters[\"datasource_name\"]\n        self.elasticsearch_host = self.job_execution_parameters[\"elasticsearch_host\"]\n        self.bucket = self.job_execution_parameters[\"source\"][\"bucket\"]\n        self.data_path_key = self.job_execution_parameters[\"source\"][\"data_path_key\"]\n        self.bucket_key = \"s3://{}{}\".format(self.bucket, self.data_path_key)\n        self.helper_files = self.job_execution_parameters[\"helper_files\"]\n\n    def getAll(self):\n        configurations = vars(self)\n        result = []\n        for conf in configurations:\n            result.append((str(conf), str(configurations[conf])))\n        return result\n\n    def printAll(self):\n        parameters = self.getAll()\n        for param in parameters:\n            logger.debug(param)\n\n\ndef initialize_job(job_execution_parameters):\n    return MapHelper(job_execution_parameters)\n\n\ndef generate_Additional_Fields(geoId, vintage, variableCode):\n    zipCode = geoId.split('US')[1]\n    code = variableCode.split('_')\n    id = '{}-{}-{}'.format(zipCode, vintage, variableCode)\n    return {'id': id, 'zipCode': zipCode, 'variableCode': code[0], 'variableCodeType': code[1]}\n\n\ndef acs_formatter(x, sc):\n    additional_Fields = generate_Additional_Fields(x['geoId'],\n                                                   x['vintage'],\n                                                   x['variableCode'])\n    if x['vintage'] == 2018:\n        calculated_value = create_calculated_value(additional_Fields['variableCode'],\n                                                   additional_Fields['variableCodeType'],\n                                                   additional_Fields['zipCode'],\n                                                   x['vintage'],\n                                                   x['estimateValue'],\n                                                   sc)\n    else:\n        calculated_value = x['estimateValue']\n\n    id = additional_Fields['id']\n    return (id, json.dumps({\n        'id': id,\n        'geoId': additional_Fields['zipCode'],\n        'variableCodeType': additional_Fields['variableCodeType'],\n        'variableCode': additional_Fields['variableCode'],\n        'estimateValue':  x['estimateValue'],\n        'calculatedValue': calculated_value,\n        'vintage':  x['vintage']\n    })\n    )\n\n\ndef create_calculated_value(variableCode, variableCodeType, zipCode, vintage, estimatedValue, sc):\n    if(variableCode == \"B25119\"):\n        return estimatedValue\n    if(variableCode == \"B25064\"):\n        query = {\n            \"query\": {\n                \"bool\": {\n                    \"filter\": [\n                     {\n                         \"bool\": {\n                             \"must\": [\n                                 {\n                                     \"match\": {\n                                         \"variableCode\": \"B25119\"\n                                     }\n                                 },\n                                 {\n                                     \"match\": {\n                                         \"variableCodeType\": \"003\"\n                                     }\n                                 },\n                                 {\n                                     \"match\": {\n                                         \"geoId\": zipCode\n                                     }\n                                 },\n                                 {\n                                     \"match\": {\n                                         \"vintage\": vintage\n                                     }\n                                 }\n                             ]\n                         }\n                     }\n                    ]\n                }\n            }\n        }\n        es_read_conf = {\n            \"es.nodes\": \"https://vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\n            \"es.port\": \"443\",\n            \"es.resource\": \"pyxis-demographics/demographics\",\n            \"es.nodes.wan.only\": \"true\",\n            \"es.query\": json.dumps(query)\n        }\n        es_rdd = sc.newAPIHadoopRDD(\n            inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n            keyClass=\"org.apache.hadoop.io.NullWritable\",\n            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n            conf=es_read_conf)\n        try:\n            return (12*estimatedValue)/es_rdd.collect()[0][1]['estimateValue']\n        except Exception as e:\n            logger.error(\"Error in calculation{}\".format(e))\n\n\ndef main():\n    try:\n\n        # *******************uncommented the below block of code to run the job again****************\n        es_write_conf_acs = {\n            \"es.nodes\": mh.elasticsearch_host,\n            \"es.port\": \"443\",\n            \"es.net.ssl\": \"true\",\n            \"es.nodes.wan.only\": \"true\",\n            \"es.resource\": \"pyxis-demographics/demographics\",\n            \"es.input.json\": \"true\",\n            \"es.mapping.id\": \"id\"\n        }\n        # df = [{\"geoId\": \"8600000US00601\", \"vintage\": 2018, \"variableCode\": \"B25064_001\", \"estimateValue\": 363, \"marginOfErrorValue\": 40, \"estimateAnnotation\": \"\", \"marginAnnotation\": \"\"},\n        #       {\"geoId\": \"8600000US00602\", \"vintage\": 2018, \"variableCode\": \"B25064_001\", \"estimateValue\": 405,\n        #        \"marginOfErrorValue\": 35, \"estimateAnnotation\": \"\", \"marginAnnotation\": \"\"},\n        #       {\"geoId\": \"8600000US00603\", \"vintage\": 2018, \"variableCode\": \"B25064_001\", \"estimateValue\": 427,\n        #        \"marginOfErrorValue\": 20, \"estimateAnnotation\": \"\", \"marginAnnotation\": \"\"},\n        #       {\"geoId\": \"8600000US00606\", \"vintage\": 2018, \"variableCode\": \"B25064_001\", \"estimateValue\": 312,\n        #        \"marginOfErrorValue\": 70, \"estimateAnnotation\": \"\", \"marginAnnotation\": \"\"},\n        #       {\"geoId\": \"8600000US00610\", \"vintage\": 2018, \"variableCode\": \"B25064_001\", \"estimateValue\": 410,\n        #        \"marginOfErrorValue\": 45, \"estimateAnnotation\": \"\", \"marginAnnotation\": \"\"}\n        #       ]\n        df = sh.ssc.read.json(\"{}/{}\".format(mh.bucket_key, \"B25064\"))\n        # formattedata = df.rdd.map(lambda x: acs_formatter(x, sh.ssc))\n\n        formattedata = []\n        for row in df.rdd.toLocalIterator():\n            obj = acs_formatter(row, sh.sc)\n            formattedata.append(obj)\n        # for row in df:\n        #     obj = acs_formatter(row, sh.sc)\n        #     formattedata.append(obj)\n\n        formatedRdd = sh.sc.parallelize(formattedata)\n        formatedRdd.saveAsNewAPIHadoopFile(\n            path='-',\n            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n            keyClass=\"org.apache.hadoop.io.NullWritable\",\n            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n            conf=es_write_conf_acs)\n\n        # formattedata.saveAsNewAPIHadoopFile(\n        #     path='-',\n        #     outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n        #     keyClass=\"org.apache.hadoop.io.NullWritable\",\n        #     valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n        #     conf=es_write_conf_acs)\n        # ***********************************\n\n        logger.info(\"Job {} finished indexing acs data\".format(job_name))\n    except Exception as e:\n        logger.error(\"Error {}\".format(e))\n        raise e\n    finally:\n        # sh.stop_spark()\n        print(\"Done!!!\")\n\n\nif __name__ == \"__main__\":\n    main()\n","user":"anonymous","dateUpdated":"2020-08-19T11:14:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597758351903_-71873132","id":"20200818-134551_1131640118","dateCreated":"2020-08-18T13:45:51+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:502"},{"text":"%spark.pyspark\nrequrired_income_data=[]\ndef fetch_income_data_for_calculation():\n    query = {\n        \"query\": {\n            \"bool\": {\n                \"filter\": [\n                    {\n                        \"bool\": {\n                            \"must\": [\n                                {\n                                    \"match\": {\n                                        \"variableCode\": \"B25119\"\n                                    }\n                                },\n                                {\n                                    \"match\": {\n                                        \"variableCodeType\": \"003\"\n                                    }\n                                },\n                                {\n                                    \"match\": {\n                                        \"vintage\": 2018\n                                    }\n                                }\n                            ]\n                        }\n                    }\n                ]\n            }\n        }\n    }\n    es_read_conf = {\n        \"es.nodes\": \"https://vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\n        \"es.port\": \"443\",\n        \"es.resource\": \"pyxis-demographics/demographics\",\n        \"es.nodes.wan.only\": \"true\",\n        \"es.query\": json.dumps(query)\n    }\n    es_rdd = sc.newAPIHadoopRDD(\n        inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n        keyClass=\"org.apache.hadoop.io.NullWritable\",\n        valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n        conf=es_read_conf)\n    requrired_income_data=es_rdd.map(lambda x: x[1])","user":"anonymous","dateUpdated":"2020-08-19T11:17:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597835686205_-1415471491","id":"20200819-111446_215583944","dateCreated":"2020-08-19T11:14:46+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:504","dateStarted":"2020-08-19T11:16:53+0000","errorMessage":""},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2020-08-19T11:16:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597835813016_-1420914974","id":"20200819-111653_523687927","dateCreated":"2020-08-19T11:16:53+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1425"}],"name":"acs_pyxis","id":"2FHE3XPPD","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
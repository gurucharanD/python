{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591189226831_-427826617","id":"20200603-130026_1672067123","dateCreated":"2020-06-03T13:00:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1768","dateUpdated":"2020-06-03T15:26:30+0000"},{"text":"%spark.pyspark\n\n#Get the Parameter and Secret\nimport os\nimport urllib.parse\nimport boto3\nimport datetime\nimport json\nimport random\nimport shutil\nimport time\n#import urllibos.environ['ParameterName'] from botocore.vendored import requests\nfrom botocore.exceptions import ClientError\nfrom io import BytesIO\nfrom datetime import date, timedelta\n\ns3_parameter = boto3.client('ssm',region_name='us-east-1')\n\ndef serialize_date_time(date_val):\n    \"\"\"\n    :param date_val: date\n    :return: string\n    \"\"\"\n    try:\n        if isinstance(date_val, datetime.datetime):\n            return str(date_val)\n    except Exception as e:\n        print(\"Failed serializing datetime: {}\".format(e))\n        \n\ndef get_secret(secret_name, region_name):\n    \"\"\"\n    :param secret_name: string\n    :param region_name: string\n    :return: json\n    \"\"\"\n    try:\n        session = boto3.session.Session()\n        client = session.client(service_name='secretsmanager', region_name=region_name)\n        secret_value = client.get_secret_value(SecretId=secret_name)\n        return secret_value['SecretString']\n    except Exception as e:\n        print('Unable to establish a secret response from AWS {}'.format(e))\n        raise e\n\nclass ParameterValues:\n    # todo add ability to pass in a parameter value (local vs cloud branch vs cloud)\n    def __init__(self):\n        self.param_name = \"\"\n        self.parameter_value_str = \"\"\n        self.parameter_value_dict = {}\n        try:\n            ParamName = \"/dev-datagov-stage-ps/datasource/acs_DEBUG\"\n        except KeyError:\n            print('Missing environment variable required for execution: ParameterName')\n            raise e\n        try:\n            parameters = s3_parameter.get_parameter(Name=ParamName, WithDecryption=True)\n            #print(parameters)\n            json_dump = json.dumps(parameters, default=serialize_date_time)\n            json_load = json.loads(json_dump)\n            # gets the string json object out of the dictionary object\n            self.parameter_value_str = json_load['Parameter']['Value']\n            global parameter_value_dict\n            # have to use json.loads again to recognize the string json object as a dictionary so we can parse out the parameter values\n            parameter_value_dict = json.loads(self.parameter_value_str)\n            \n            # print (parameter_value_dict['regionName'], \"|\", parameter_value_dict['secretName'])\n            #secret_login = json.loads(get_secret(region_name=parameter_value_dict['regionName'], secret_name=parameter_value_dict['secretName']))\n\n        except Exception as e:\n            print('Failed to extract the value from the parameter \\\n                object {} ! {}'.format(self.parameter_value_str, e))\n            raise e\n            \n#             paramater = \"\"\"{\n#   \"secretName\": \"dev-datagov-identification-sm/messagestore/common/credential\",\n#   \"acs_path\": \"s3://271849239056-dev-datagov-stage-s3/datasource=acs/source-files/\",\n#   \"acs_geonames_file_path\": \"s3://271849239056-dev-datagov-stage-s3/datasource=acs/source-static-data/ACS2EODSGeonames.csv\",\n#   \"acs_variable_file_path\": \"s3://271849239056-dev-datagov-stage-s3/datasource=acs/source-static-data/ACSVariableNames.csv\",\n#   \"postgres_url\": \"jdbc:postgresql://dg-dev-message-store-instance-1.cgudqigyytx1.us-east-1.rds.amazonaws.com:5432/message_store\",\n#   \"postgres_user\": \"message_store\",\n#   \"regionName\": \"us-east-1\"\n# }\"\"\"\n        \ntry:\n    params = ParameterValues()\nexcept Exception as e:\n        print(\"Error {}\".format(e))\n        exit(1)\nprint(\"Done: Parameter and Secret retreived\")\n            \nprint('Spark Context Version: {}'.format(sc.version))\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom pyspark.sql.types import DataType, StructType, StructField\n\ntry:\n   \n    acs_path =\"S3://271849239056-dev-datagov-stage-s3/Pyxis/\"\n\n\n    ACS_RAW = sqlContext.read.json(path=acs_path)\n                       \n    \n    # z.show(ACS_RAW.select(\"GeoID\",\"vintage\",\"group\",\"variable\",\"E\",\"M\",\"EA\",\"MA\"))\n    # z.show(ACS_RAW.select(\"GeoID\").where(col(\"GeoID\") == \"1400000US01001020100\"))\n    # z.show(ACS_RAW.collect())\n    # z.show(ACS_RAW.show())\n                       \n    # print(\"typeof acs raw is \",type(ACS_RAW))\n    # print(\"typeof acs raw select \",ACS_RAW.select(\"GeoID\",\"vintage\",\"group\",\"variable\",\"E\",\"M\",\"EA\",\"MA\"))\n    \n    ACS_Data=ACS_RAW.collect()\n    \n    # for column in ACS_RAW.select(\"GeoID\",\"vintage\",\"group\",\"variable\",\"E\",\"M\",\"EA\",\"MA\"):\n    # i = 0\n    # builk_body=[]\n    # for column in ACS_Data:\n    #     # i = i +1\n    #     # print('column type in acs_raw',type(column),column)\n    #     # for item in column:\n    #         # print('this is item ',type(item),item)\n    #     # if i == 10:\n    #     #     break\n        \n    #     # print( column.geoId , column.vintage,column.vintage,type(column.geoId) ,type( str(column.vintage)),type(column.vintage) )\n    #     index= {\n    #         '_index': 'pyxis-acs',\n    #         '_type': 'acs',\n    #         '_id': column.geoId + str(column.vintage) + column.variableCode}\n    #     builk_body.append(index)\n    #     builk_body.append(column)\n    #     # print(index)\n    # print(builk_body)\n        \n        \nexcept KeyError:\n            print('ACS_RAW dataframe did not load properly')\n            raise e\n            \n# print(ACS_RAW.count())\nprint(\"Done: ACS_RAW data frame is available\")\n\nprint(acs_path)\n\nz.show(ACS_RAW)","user":"anonymous","dateUpdated":"2020-06-03T15:26:30+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-10-66-198-209.ec2.internal:4040/jobs/job?id=71","http://ip-10-66-198-209.ec2.internal:4040/jobs/job?id=72","http://ip-10-66-198-209.ec2.internal:4040/jobs/job?id=73"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1591163813921_-1688458303","id":"20200601-090702_1131329536","dateCreated":"2020-06-03T05:56:53+0000","dateStarted":"2020-06-03T12:55:39+0000","dateFinished":"2020-06-03T12:55:50+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1769"},{"text":"%spark.pyspark\n\nfrom pyspark.sql.functions import col\n\n# reader = spark \\\n#     .read \\\n#     .format(\"org.elasticsearch.spark.sql\") \\\n#     .option(\"es.read.metadata\", \"true\") \\\n#     .option(\"es.nodes.wan.only\",\"true\") \\\n#     .option(\"es.port\",\"443\") \\\n#     .option(\"es.net.ssl\",\"true\") \\\n#     .option(\"es.mapping.date.rich\", \"false\") \\\n#     .option(\"es.read.field.as.array.include\", \"\") \\\n#     .option(\"es.nodes\", \"vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\")\n#     .options()\n    \n\n\n# for row in ACS_RAW.rdd.toLocalIterator():\n#     print(row)\n# z.show(ACS_RAW)\n# ACS_RAW.saveToEs(\"dataframejsonindex\")\n# pyxis_market_dashboard_df = reader.load(\"pyxis-marketdashboard\").limit(1000)\n\n# Print('pyxis_market_dashboard_df JOB done')\n\n# mydf = pyxis_market_dashboard_df \\\n#     .select(col(\"MarketCode\"),\n#     col(\"MarketName\"),\n#     col(\"SubmarketCode\"),\n#     col(\"SubmarketName\"))\n    \n    # print('mydf JOB done')\n\n# z.show(mydf)","user":"anonymous","dateUpdated":"2020-06-03T15:26:30+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"estimateAnnotation":"string","estimateValue":"string","geoId":"string","marginAnnotation":"string","marginOfErrorValue":"string","variableCode":"string","vintage":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 36: rdd3 = ACS_RAW.rdd.map(addID)\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3324175376194000706.py\", line 375, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 36, in <module>\nNameError: name 'addID' is not defined\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-10-66-198-209.ec2.internal:4040/jobs/job?id=96"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1591163813922_-490814977","id":"20200601-090721_305615771","dateCreated":"2020-06-03T05:56:53+0000","dateStarted":"2020-06-03T13:59:35+0000","dateFinished":"2020-06-03T13:59:43+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:1770"},{"text":"%spark.pyspark\nimport json\nimport hashlib\nimport re\nacs_path =\"S3://271849239056-dev-datagov-stage-s3/Pyxis/\"\n# ACS_RAW = sqlContext.read.json(path=acs_path)\ndata=['83.149.9.216 - - [17/May/2015:10:05:03 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"']\n\nACS_RAW = spark.sparkContext.parallelize(data)\n\n\n# ACS_RAW.show()\n\nes_write_conf = {\n        \"es.nodes\" : \"vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\n        \"es.port\" : \"443\",\n        \"es.resource\" : 'walker/apache',\n        \"es.input.json\": \"yes\",\n        \"es.nodes.wan.only\":\"true\",\n            \"es.read.metadata\": \"true\",\n    \"es.nodes.wan.only\":\"true\",\n    \"es.port\":\"443\",\n    \"es.net.ssl\":\"true\",\n    \"es.mapping.date.rich\": \"false\",\n    \"es.read.field.as.array.include\": \"\",\n       \"es.mapping.id\": \"doc_id\"\n    }\nregex='^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+)\\s?(\\S+)?\\s?(\\S+)?\" (\\d{3}|-) (\\d+|-)\\s?\"?([^\"]*)\"?\\s?\"?([^\"]*)?\"?$'\np=re.compile(regex)\n\ndef parse(str):\n    s=p.match(str)\n    d = {}\n    d['ip']=s.group(1)\n    d['date']=s.group(4)\n    d['operation']=s.group(5)\n    d['uri']=s.group(6)\n    return d  \n    \ndef addId(data):\n    j=json.dumps(data).encode('ascii', 'ignore')\n    # data['doc_id'] = hashlib.sha224(j).hexdigest()\n    return (12345, json.dumps(data))\n    \n# rdd3 = ACS_RAW.rdd.map(addId)\nrdd3 = ACS_RAW.map(parse)\n# rdd3.take(1)\nrdd3 = sc.parallelize(rdd3)\nrdd3 = rdd3.map(lambda elem: list(elem))\nrdd3.take(1)\n\n# rdd3.saveAsNewAPIHadoopFile(\n#         path='-',\n#         outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\", \n#         keyClass=\"org.apache.hadoop.io.NullWritable\",\n#         valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n#         conf=es_write_conf)\n\n","user":"anonymous","dateUpdated":"2020-06-03T15:49:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 47: rdd3 = sc.parallelize(rdd3)\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3324175376194000706.py\", line 375, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 47, in <module>\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 525, in parallelize\n    c = list(c)    # Make it a list so we can compute its length\nTypeError: 'PipelinedRDD' object is not iterable\n"}]},"apps":[],"jobName":"paragraph_1591192650794_281781411","id":"20200603-135730_1276478248","dateCreated":"2020-06-03T13:57:30+0000","dateStarted":"2020-06-03T15:49:59+0000","dateFinished":"2020-06-03T15:49:59+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:1771"},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2020-06-03T15:26:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591192983250_-324495777","id":"20200603-140303_2021249394","dateCreated":"2020-06-03T14:03:03+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1772"}],"name":"acs_pyxis","id":"2FC7BF217","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597902035743_-2029438154","id":"20200820-054035_576797309","dateCreated":"2020-08-20T05:40:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:547","text":"%spark.pyspark\nimport os\nimport urllib.request\nimport zipfile\nimport json\nimport logging\njob_name = \"acs_data\"\n\nformatter = '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - ln%(lineno)d - %(message)s'\nlogging.basicConfig(format=formatter)\nlogger = logging.getLogger(job_name)\nlogger.setLevel(logging.DEBUG)\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('acsdata').getOrCreate()\nes_hadoop = urllib.request.URLopener()\nes_hadoop.retrieve(\"http://download.elastic.co/hadoop/elasticsearch-hadoop-6.1.1.zip\", \"es-hadoop.zip\")\nwith zipfile.ZipFile(\"es-hadoop.zip\",\"r\") as zip_ref:\n    zip_ref.extractall()\n    \nos.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-6.1.1/dist/elasticsearch-spark-20_2.11-6.1.1.jar pyspark-shell'\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('acsdata').getOrCreate()\n\nes_write_conf_acs = {\n    \"es.nodes\" : \"https://vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\n    \"es.port\" : \"443\",\n    \"es.net.ssl\": \"true\",\n    \"es.nodes.wan.only\": \"true\",\n    \"es.resource\" : \"acsdata/data\",\n    \"es.input.json\" : \"yes\",\n\t\"es.mapping.id\": \"id\"\n}\n\ndf = spark.read.json('s3://271849239056-dev-datagov-stage-s3/datasource=acs/acs-pyxis/B25064')\n\n","dateUpdated":"2020-08-20T05:42:52+0000","dateFinished":"2020-08-20T05:43:01+0000","dateStarted":"2020-08-20T05:42:52+0000","results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-10-66-198-116.ec2.internal:4041/jobs/job?id=60629"],"interpreterSettingId":"spark"}}},{"text":"%spark.pyspark\ndef generate_Additional_Fields(geoId, vintage, variableCode):\n    zipCode = geoId.split('US')[1]\n    code = variableCode.split('_')\n    id = '{}-{}-{}'.format(zipCode, vintage, variableCode)\n    return {'id': id, 'zipCode': zipCode, 'variableCode': code[0], 'variableCodeType': code[1]}","user":"anonymous","dateUpdated":"2020-08-20T05:43:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597902095922_494875700","id":"20200820-054135_324240344","dateCreated":"2020-08-20T05:41:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:632","dateFinished":"2020-08-20T05:43:08+0000","dateStarted":"2020-08-20T05:43:08+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\ndef acs_formatter(x,incomedata_rdd):\n    additional_Fields = generate_Additional_Fields(x['geoId'],\n                                                   x['vintage'],\n                                                   x['variableCode'])\n    print('--acs_formatter------>',x)\n    if x['vintage'] == 2018:\n        print('in vintageeeeeeeee')\n        calculated_value = create_calculated_value(additional_Fields['variableCode'],\n                                                   additional_Fields['variableCodeType'],\n                                                   additional_Fields['zipCode'],\n                                                   x['vintage'],\n                                                   x['estimateValue'],\n                                                   incomedata_rdd\n                                                   )\n    else:\n        calculated_value = x['estimateValue']\n\n    id = additional_Fields['id']\n    return (id, json.dumps({\n        'id': id,\n        'geoId': additional_Fields['zipCode'],\n        'variableCodeType': additional_Fields['variableCodeType'],\n        'variableCode': additional_Fields['variableCode'],\n        'estimateValue':  x['estimateValue'],\n        'calculatedValue': calculated_value,\n        'vintage':  x['vintage']\n    })\n    )","user":"anonymous","dateUpdated":"2020-08-20T06:10:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597902188943_-923846263","id":"20200820-054308_1738367473","dateCreated":"2020-08-20T05:43:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:804","dateFinished":"2020-08-20T06:10:37+0000","dateStarted":"2020-08-20T06:10:37+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\ndef create_calculated_value(variableCode, variableCodeType, zipCode, vintage, estimatedValue,incomedata_rdd):\n    if(variableCode == \"B25119\"):\n        return estimatedValue\n    if(variableCode == \"B25064\"):\n        try:\n            income_test=0\n            income_test = incomedata_rdd.filter(lambda x: x['geoId'] == zipCode)\n            return (12*estimatedValue)/income_test.collect()[0]['estimateValue']\n        except Exception as e:\n            logger.error(\"Error in calculation{}\".format(e))\n","user":"anonymous","dateUpdated":"2020-08-20T05:45:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597902203034_-287687675","id":"20200820-054323_1039986743","dateCreated":"2020-08-20T05:43:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:876","dateFinished":"2020-08-20T05:45:19+0000","dateStarted":"2020-08-20T05:45:19+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\ndef main():\n    try:\n        logger.info(\"Job {} started indexing acs data\".format(job_name))\n        \n\n        # *******************uncommented the below block of code to run the job again****************\n                \n        es_write_conf_acs = {\n            \"es.nodes\": \"https://vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\n            \"es.port\": \"443\",\n            \"es.net.ssl\": \"true\",\n            \"es.nodes.wan.only\": \"true\",\n            \"es.resource\": \"pyxis-demographics/demographics\",\n            \"es.input.json\": \"true\",\n            \"es.mapping.id\": \"id\"\n        }\n\n        incomedata_rdd=fetch_income_data_for_calculation(sc)\n        # incomedata_rdd=[]\n        # incomedata_rdd = sc.parallelize(requrired_income_data)\n        # print('main  type(incomedata_rdd) ',type(incomedata_rdd))\n        # print(incomedata_rdd.collect()[0])\n        # print('dfffff  ',df.rdd.count(),type(df))\n        \n        # # formatedRdd = df.rdd.map(lambda x: acs_formatter(x))\n        # formattedata = []\n        formatedRdd = []\n        \n        # fetch_income_data_for_calculation(sc)\n        # incomedata_rdd = sc.parallelize(requrired_income_data)\n\n        # for row in df.rdd.toLocalIterator():\n        #     obj = acs_formatter(row,incomedata_rdd)\n        #     formattedata.append(obj)\n\n        formatedRdd = df.rdd.map(lambda x:acs_formatter(x,incomedata_rdd))\n        \n        formatedRdd.saveAsNewAPIHadoopFile(\n            path='-',\n            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n            keyClass=\"org.apache.hadoop.io.NullWritable\",\n            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n            conf=es_write_conf_acs)\n            \n        logger.info(\"Job {} inserted all  records shutting down\".format(job_name))\n\n        # ***********************************\n\n        logger.info(\"Job {} finished indexing acs data\".format(job_name))\n    except Exception as e:\n        logger.error(\"Error {}\".format(e))\n        raise e\n    finally:\n        print(\"Done!!!\")","user":"anonymous","dateUpdated":"2020-08-20T06:16:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597902343970_875556286","id":"20200820-054543_1155974489","dateCreated":"2020-08-20T05:45:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1020","dateFinished":"2020-08-20T06:16:59+0000","dateStarted":"2020-08-20T06:16:59+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\ndef fetch_income_data_for_calculation(spx):\n    print('fetch_income_data_for_calculation',spx)\n    requrired_income_data=[]\n    query = {\n        \"query\": {\n            \"bool\": {\n                \"filter\": [\n                    {\n                        \"bool\": {\n                            \"must\": [\n                                {\n                                    \"match\": {\n                                        \"variableCode\": \"B25119\"\n                                    }\n                                },\n                                {\n                                    \"match\": {\n                                        \"variableCodeType\": \"003\"\n                                    }\n                                },\n                                {\n                                    \"match\": {\n                                        \"vintage\": 2018\n                                    }\n                                }\n                            ]\n                        }\n                    }\n                ]\n            }\n        }\n    }\n    es_read_conf = {\n        \"es.nodes\": \"https://vpc-elasticsearch-polarisapps-qc-kttkced2njswnhaa2n4mbou2cq.us-east-1.es.amazonaws.com\",\n        \"es.port\": \"443\",\n        \"es.resource\": \"pyxis-demographics/demographics\",\n        \"es.nodes.wan.only\": \"true\",\n        \"es.query\": json.dumps(query)\n    }\n    print('fetch_income_data_for_calculation----->',es_read_conf)\n    es_rdd = spx.newAPIHadoopRDD(\n        inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n        keyClass=\"org.apache.hadoop.io.NullWritable\",\n        valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n        conf=es_read_conf)\n    print('fetch_income_data_for_calculation----->',es_rdd)\n\n    print('fetch_income_data_for_calculation----->')\n\n    for row in es_rdd.toLocalIterator():\n        requrired_income_data.append(row[1])\n        \n    print('fetch_income_data_for_calculation-88888888888888---->')\n\n    incomedata_rdd=[]\n    incomedata_rdd = sc.parallelize(requrired_income_data)\n    print('fetch_income_data_for_calculation enddddddd')\n    return incomedata_rdd","user":"anonymous","dateUpdated":"2020-08-20T06:21:06+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597904179648_-1621871192","id":"20200820-061619_676179722","dateCreated":"2020-08-20T06:16:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1806","dateFinished":"2020-08-20T06:21:06+0000","dateStarted":"2020-08-20T06:21:06+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\nfetch_income_data_for_calculation(sc)","user":"anonymous","dateUpdated":"2020-08-20T06:20:13+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597903891108_-1609758394","id":"20200820-061131_1554559047","dateCreated":"2020-08-20T06:11:31+0000","status":"ABORT","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1099","dateFinished":"2020-08-20T06:20:18+0000","dateStarted":"2020-08-20T06:20:13+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"fetch_income_data_for_calculation <SparkContext master=yarn appName=Zeppelin>\n"},{"type":"TEXT","data":"Fail to execute line 1: fetch_income_data_for_calculation(sc)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: org.apache.spark.SparkException: Job 60635 cancelled part of cancelled job group zeppelin-2FFP894G6-20200820-061131_1554559047\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1976)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:946)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:946)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:946)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2231)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1364)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:302)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7518645077676131532.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 44, in fetch_income_data_for_calculation\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 777, in newAPIHadoopRDD\n    jconf, batchSize)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 67, in deco\n    e.java_exception.getStackTrace()))\n  File \"/usr/lib64/python3.6/_collections_abc.py\", line 883, in __iter__\n    v = self[i]\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\", line 191, in __getitem__\n    return self.__compute_item(key)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\", line 173, in __compute_item\n    answer = self._gateway_client.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 991, in send_command\n    self._give_back_connection(connection)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 942, in _give_back_connection\n    self.deque.append(connection)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 278, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-10-66-198-116.ec2.internal:4041/jobs/job?id=60635"],"interpreterSettingId":"spark"}}},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2020-08-20T06:11:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597903904135_-1860386057","id":"20200820-061144_1188816455","dateCreated":"2020-08-20T06:11:44+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1171"}],"name":"pyxis/b25064","id":"2FFP894G6","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}